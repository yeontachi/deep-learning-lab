## 단어 임베딩 공간 예시
Code: Transformer_Self-Attention_Value_examples.py
Image: ![Alt text](/2025-08-14-Transformer/LLM/WordEmbedding.png)

그림을 확인해보면 단어 임베딩 공간을 2차원으로 축소한 시각화 이미지이다.

원래 단어 벡터들은 5차원 공간에 있었는데, PCA를 통해 2차원으로 줄였다.
비슷한 의미를 가진 단어들이 **서로 가까운 위치**에 모이고,
멀리 있는 단어들은 **의미적으로 덜 관련**있다.

즉, 모델이 만든 "공간"에서 각 좌표는 데이터 학습을 통해 자동으로 형성된 의미 축이며, 
현재 보이는 건 고차원 공간의 단면이고, 여기서 벡터의 방향과 거리가 실제로 단어들의 의미적 
관계를 표현한다.