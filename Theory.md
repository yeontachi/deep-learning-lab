## 신경망(Neural Network)이란?
신경망은 **인간의 뇌 구조**에서 영감을 받은 **기계학습 모델**로, 입력 데이터를 받아 여러 층을 거쳐 **출력값을 예측**하는 구조이다. 특히 이미지, 음성, 자연어, 숫자 등 다양한 데이터를 처리할 수 있고, 요즘 우리가 말하는 **딥러닝(Deep Learning)**의 가장 기본적인 구성 요소이다.

**신경망(Neural Network)**이란 여러 개의 연산 노드(=뉴런, neuron)들이 층(Layer)으로 구성되어 있고, 이 노드들이 **가중치(weight)와 편향(bias)**를 바탕으로 데이터를 처리하는 모델이다.

### 퍼셉트론(Perceptron)
**퍼셉트론**은 가장 기본적인 인공 뉴런이며, 입력값들과 각각의 가중치를 곱해서 모두 더한 후, 일정기준(임계값)을 넘으면 1, 아니면 0을 출력하는 간단한 구조이다. 아래는 **OR**연산을 하는 퍼셉트론이다.

![Alt text](/images/Perceptron.png)

하지만 퍼셉트론은 기본적으로 선형으로 표현되는 결정 경계만을 설정하는데, XOR는 선형 경계로 부류들을 분리할 수 없는 문제이다. 이러한 문제를 **선형 분리 불가 문제(Linearly inseparable problem)**라고 한다.

![Alt text](/images/linear_separable.png)

### 다층 퍼셉트론(Multilayer Perceptron, MLP)
퍼셉트론의 한계인 선형 분리 불가 문제를 해결하기 위해 나온 것이 **다층 퍼셉트론**이다.(즉, 비선형 문제도 처리할 수 있게 만든 인공 신경망의 기본 형태)

**다층 퍼셉트론**은 **입력층(Input Layer), 은닉층(Hidden Layer), 출력층(Output Layer)**으로 구성된 인공 신경망이다. 각 층은 여러 개의 노드(뉴런)으로 이루어져 있으며, 각 노드는 이전 층의 모든 노드와 연결되어 있다.

![Alt text](/images/multilevel.png)

### 신경망(Neural Network, 네트워크)
 입력층(Input Layer) -> 은닉층(Hidden Layer) -> 출력층(Output Layer)으로 구성된 구조이다. 각각의 층(layer)은 여러 노드(node) 또는 뉴런(neuron)으로 구성된다. 가중치(Weight)와 편향(bias)를 통해 입력값을 선형 변환하고, 활성화 함수(Activation Function)를 통해 비선형성을 부여한다.

 **예시**
  - 입력층 : 784개(MNIST의 28x28 이미지)
  - 은닉층 : 128개의 뉴런
  - 출력층 : 10개(0~9)
## 딥러닝(Deep Learning)이란?
**딥러닝**은 **인공신경망(Artificial Neural Network)**을 기반으로 한 **기계학습(Machine Learning)** 기법 중 하나로, **여러 개의 은닉층(hidden layer)**을 쌓아 깊은 구조를 만든 모델을 의미한다.

기존의 단일 퍼셉트론이나 얕은 신경망으로는 처리하기 어려운 **복잡한 데이터의 패턴과 추상적인 특징**을 자동으로 학습할 수 이씨게 해준다.

![Alt text](/images/DeepLearning.png)

 - **다층 구조** : 입력층 -> 여러 은닉층 -> 출력층으로 구성
 - **비선형 변환** : 각 층에서 활성화 함수를 통해 복잡한 함수 근사를 수행
 - **표현 학습(Representation Learning)** : 사람이 직접 특징을 설계하지 않아도, 모델이 데이터로부터 중요한 표현을 자동으로 학습함
 - **대규모 데이터 + GPU 병렬 처리에 최적화됨**

## 딥러닝 핵심 용어 중심 정리

### Batch / Batch Size
 전체 학습 데이터를 한 번에 모두 처리하는 것이 아니라, 일부만 나눠서 처리할 때의 **그 일부 묶음**을 **"배치(batch)"**라고 한다.

 **배치 사이즈(batch size)**는 한 번에 네트워크에 넣는 데이터의 수를 의미한다.

 예를 들어, 전체 학습 데이터가 10000개, 배치 사이즈가 100이면, 한 번에 100개씩 학습 = 총 100번 학습 반복 -> 1에폭

 **종류**
  - **Mini-batch Gradient Descent** : 가장 일반적, batch size = 보통 32, 64, 128 등
  - **Stochastic Gradient Descent(SGD)** : batch size = 1
  - **Batch Gradient Descent** : 전체 데이터

### Epoch
 전체 학습 데이터를 한 번 모두 사용해서 학습을 완료한 횟수
 예를 들어, 1에폭이면 전체 데이터를 한 번 다 학습. 에폭을 늘리면 학습이 더 깊어진다.

### Parameter
 신경망이 학습을 통해 조정하는 가중치(Weight)와 편향(bias)를 통칭한다. 학습이란 곧 파라미터를 **손실 함수 기준으로 업데이트**하는 것이다.

### 순전파(Forward Propagation)
입력 데이터를 네트워크에 넣어 출력을 계산하는 과정이다.

```
입력값 → 가중치 곱 → 활성화 함수 → 다음 층 … 반복 → 최종 출력값
```

### 역전파(Back Propagation)
출력값과 정답의 오차를 기반으로 **각 가중치의 변화량(기울기, gradient)**을 계산하여 **가중치를 업데이트**하는 과정

손실 함수를 미분하여, 각 층에 대해 역방향으로 gradient를 계산한다.

### 손실 함수(Loss Function)
 모델의 예측값과 실제 정답 간의 오차를 수치로 표현한 함수

 예시: 
  - 분류

### 활성화 함수(Activation Function)
 선형 변환된 입력을 비선형적으로 바꾸는 함수

 **종류**
  - ReLU, Sigmoid, Tanh, Softmax

### 과적합/부적합
 과적합:  모델이 학습 데이터에 지나치게 맞추어진 상태로, 훈련 데이터엔 잘 맞지만, 테스트 데이터엔 성능이 떨어지는 현상(너무 외워버림)
 부적합: 훈련 데이터조차 잘 못 맞추는 상태(모델이 너무 단순함)
![Alt text](/images/Overfiting.png)

#### 과적합 문제 해결 방법
 - **드롭 아웃(DropOut)** : 학습 중 일부 뉴런을 확률적으로 꺼서 모델 일반화를 돕는 정규화 기법
    - 미니 배치(mini-batch)나 학습 주기(epoch) 드롭아웃 할 노드들을 새롭게 선택하여 학습한다.
    - 추론을 할 때는 드롭아웃을 하지 않고 전체 학습된 신경망을 사용하여 출력을 계산한다.

 ![Alt text](/images/DropOut.png)

 - **미니 배치(Minibatch)** : 전체 학습 데이터를 일정 크기로 나누어 놓은 것으로, 학습 데이터가 큰 경우에는 미니 배치 단위로 학습한다. 미니배치를 사용하면 데이터에 포함된 오류에 대해 둔감한 학습이 가능하다.(과적합 완화에 도움)

 - **정규화(Reqularization)** : 큰 가중치 억제


### 옵티마이저(Optimizer)
 손실 함수의 값을 줄이기 위해 파라미터를 어떻게 조정할지 결정하는 알고리즘
  
 **종류** 
  - SGD(확률적 경사 하강법)
  - Adam: 가장 널리 사용됨, 모맨텀 + 적용 학습률
  - RMSprop, Adagrad 등

### 학습률(Learning Rate)
 옵티마이저가 파라미터를 얼마나 크게 변화시킬지 결정하는 값으로, 너무 크면 발산, 너무 작으면 학습이 느리다.

### 데이터셋 분리(Training / Validation / Test Set)
 - **Training Set** : 모델 학습용
 - **Validation Set** : 하이퍼파라미터 튜닝 및 early stopping 용
 - **Test Set** : 최조 성능 평가용(실제 서비스에 가깝게 사용)

## 전체 흐름 요약
 - 데이터를 배치 단위로 나눠서
 - 에폭 수 만큼 순전파 + 역전파 반복
 - 옵티마이저로 손실 함수 값을 줄이고
 - 정규화 기법으로 과적합 방지하며
 - 최종적으로 테스트 데이터에서 좋은 성능을 내는 모델을 만드는 것
 