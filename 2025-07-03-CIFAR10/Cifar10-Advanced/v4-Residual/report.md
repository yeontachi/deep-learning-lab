## Performance

### Shuffle - Residual Block
| Epoch | Train Loss | Train Accuracy (%) | Test Loss | Test Accuracy (%) |
|-------|------------|---------------------|-----------|--------------------|
| 1     | 0.9829     | 65.03               | 0.7009    | 75.70              |
| 2     | 0.6085     | 78.88               | 0.5232    | 81.93              |
| 3     | 0.4605     | 84.07               | 0.4670    | 84.28              |
| 4     | 0.3693     | 87.33               | 0.4220    | 85.76              |
| 5     | 0.3086     | 89.38               | 0.4187    | 86.60              |
| 6     | 0.2622     | 91.05               | 0.4107    | 86.80              |
| 7     | 0.2315     | 91.97               | 0.4089    | 87.04              |
| 8     | 0.2108     | 92.78               | 0.4274    | 87.60              |
| 9     | 0.1919     | 93.31               | 0.4060    | 88.73              |
| 10    | 0.1810     | 93.89               | 0.3989    | 88.53              |
| 11    | 0.1651     | 94.41               | 0.4049    | 88.65              |
| 12    | 0.1566     | 94.69               | 0.4340    | 88.09              |
| 13    | 0.1479     | 94.96               | 0.4133    | 88.37              |
| 14    | 0.1449     | 95.06               | 0.4143    | 88.79              |
| 15    | 0.1319     | 95.55               | 0.4332    | 88.80              |

### Curriculum(Confidence Score) - Residual Block

| Stage      | Best Epoch | Train Loss | Train Accuracy (%) | Test Loss | Test Accuracy (%) |
|------------|------------|-------------|---------------------|-----------|--------------------|
| Stage 1    | 8          | 0.0908      | 96.99               | 1.2314    | 68.83              |
| Stage 2    | 8          | 0.4812      | 83.64               | 0.5215    | 82.11              |
| Stage 3    | 10         | 0.3228      | 88.85               | 0.4663    | 84.65              |
| Fine-Tune  | 10         | 0.1343      | 95.45               | 0.3976    | 89.10              |


## 성능 비교 Non-Residual, Residual

### Shuffle Learning

| Method                     | Epoch | Train Loss | Train Accuracy (%) | Test Loss | Test Accuracy (%) |
|----------------------------|--------|-------------|---------------------|-----------|--------------------|
| Shuffle + Residual Module | 15     | 0.1319      | 95.55               | 0.4332    | 88.80              |
| Shuffle Only              | 11     | 0.1960      | 93.30               | 0.4279    | 87.52              |


### Curriculum Learning

| Method          | Epoch | Train Loss | Train Accuracy (%) | Test Loss | Test Accuracy (%) |
|-----------------|--------|-------------|---------------------|-----------|--------------------|
| No Residual     | 10     | 0.3130      | 89.00               | 0.4130    | 86.85              |
| With Residual   | 10     | 0.1343      | 95.45               | 0.3976    | 89.10              |


![Alt text](/2025-07-03-CIFAR10/Cifar10-Advanced/v4-Residual/Images/Residual_In_Curriculum.png)

## 분석 
Residual Module, 또는 잔차 모듈은 2015년에 발표된 **ResNet(Residual Network)**에서 도입된 신경망 구조로, **신호가 깊은 네트워크를 통과하면서 손실되는 현상(vanishing gradient)**을 극복하기 위해 제안된 구조이다. 일반적인 CNN에서는 층이 깊어질수록 오히려 성능이 하락하거나, 학습이 어려워지는 문제가 발생했는데, Residual Module은 이러한 문제를 **identity shortcut connection(입력값을 출력에 더함)**이라는 단순하면서도 효과적인 방법으로 해결된다.

기본적인 Residual Block은 다음과 같은 구조를 가진다.

```
F(x) = Activation(Conv2d(BN(Conb2(x))))
Output = F(x) + x
```

여기서 F(x)는 convolution과 비선형 활성화(ReLU) 등을 통과한 출력이고, 이 출력에 **입력 x를 그대로 더하는 Skip connection**을 사용한다. 이를 통해 신경망은 **입력과의 차이(잔차)**만 학습하면 되므로, 훨씬 더 빠르고 안정적으로 수렴할 수 있다.

### Residual Module의 특징과 이점

잔차 모듈의 가장 큰 특징은 다음과 같다:

1. **Gradient 흐름 보존**

   * 깊은 네트워크에서도 역전파 시 gradient가 중간층까지 잘 전달됨
   * → vanishing gradient 문제 완화

2. **빠른 수렴과 안정된 학습**

   * 깊은 네트워크일수록 학습 초기에 발산하거나 수렴이 느린 문제가 있음
   * 잔차 모듈은 초기 가중치 설정이 잘못되더라도 비교적 학습이 안정적으로 진행됨

3. **특징 재사용(feature reuse)**

   * 입력 자체가 다음 층으로 그대로 전달되므로, 이전 층에서 추출한 특징을 유실하지 않고 활용 가능

4. **비교적 쉬운 구현과 확장성**

   * 구조가 단순해서 기존 CNN에 쉽게 추가할 수 있으며, 네트워크를 더 깊게 만들 때도 효과적임

---

### 실험 결과로 본 Residual Module의 효과

실험 결과는 Shuffle Training과 Curriculum Learning 두 가지 학습 방식에서 각각 **잔차 모듈의 적용 유무에 따라 성능 차이가 뚜렷하게 나타남**을 보여준다.

#### 1. Shuffle Training 비교

| Method                    | Epoch | Train Acc (%) | Test Acc (%) |
| ------------------------- | ----- | ------------- | ------------ |
| Shuffle + Residual Module | 15    | 95.55         | **88.80**    |
| Shuffle Only              | 11    | 93.30         | **87.52**    |

* 같은 조건에서 학습한 경우, **잔차 모듈을 적용한 모델이 더 빠르게 수렴**하고, 최종 정확도 역시 **약 1.3% 향상**되었다.
* 이는 Residual Block이 입력 정보 손실 없이 deeper한 모델 구성이 가능하게 하며, **더 풍부한 표현 학습**이 가능했음을 의미한다.

#### 2. Curriculum Learning 비교

| Method        | Epoch | Train Acc (%) | Test Acc (%) |
| ------------- | ----- | ------------- | ------------ |
| No Residual   | 10    | 89.00         | **86.85**    |
| With Residual | 10    | 95.45         | **89.10**    |

* Curriculum(Confidence 기반 샘플 학습) 조건에서도, **잔차 모듈을 적용한 모델은 약 2.25%의 정확도 향상**을 보였다.
* 특히 훈련 정확도가 더 높게 나왔음에도 불구하고 과적합 없이 일반화 정확도도 같이 상승했다는 점에서, **모델의 표현력과 안정성**을 동시에 확보한 것으로 해석된다.

---

### 결론: Residual Module을 사용하는 이유

잔차 모듈은 단순한 구조지만, 매우 깊은 신경망에서의 **학습 가능성, 안정성, 성능 모두를 개선**할 수 있는 강력한 도구이다. 실험 결과를 통해 우리는 다음과 같은 결론을 도출할 수 있다:

* Residual 구조는 **얕은 네트워크에서는 성능이 비슷하거나 약간 우세**하지만,
* **깊은 구조나 커리큘럼 학습처럼 학습이 점진적으로 이뤄지는 방식에서는 성능 격차가 명확하게 드러남**
* 결과적으로, Residual Module은 **정확도 향상뿐만 아니라 모델의 학습 신뢰성과 일반화 성능을 높이는 데 핵심적인 역할**을 한다.




