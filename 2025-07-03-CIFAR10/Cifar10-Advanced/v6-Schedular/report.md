## 성능

### ResidualVGGInspired model + Shuffle(Data Augmentation)
| Epoch | Train Loss | Train Accuracy (%) | Test Loss | Test Accuracy (%) |
|-------|------------|---------------------|-----------|--------------------|
| 1     | 0.9933     | 64.81               | 0.6868    | 76.44              |
| 2     | 0.6146     | 78.65               | 0.5090    | 82.46              |
| 3     | 0.4695     | 83.77               | 0.4437    | 85.18              |
| 4     | 0.3774     | 87.04               | 0.4337    | 85.55              |
| 5     | 0.3150     | 89.18               | 0.4019    | 87.09              |
| 6     | 0.2682     | 90.73               | 0.3947    | 87.45              |
| 7     | 0.2326     | 92.08               | 0.3964    | 87.24              |
| 8     | 0.2137     | 92.74               | 0.4324    | 87.21              |
| 9     | 0.1955     | 93.28               | 0.4102    | 88.10              |
| 10    | 0.1824     | 93.83               | 0.4108    | 88.19              |
| 11    | 0.1368     | 95.35               | 0.3696    | 89.59              |
| 12    | 0.1213     | 95.93               | 0.3834    | 89.41              |
| 13    | 0.1128     | 96.19               | 0.3905    | 89.71              |
| 14    | 0.1081     | 96.38               | 0.4129    | 89.50              |
| 15    | 0.1057     | 96.36               | 0.3967    | 89.86              |
| 16    | 0.0865     | 97.11               | 0.3821    | **90.21**          |

### ResidualVGGINspired model + Curriculum(Confidence Score)

| Stage      | Best Epoch | Train Loss | Train Accuracy (%) | Test Loss | Test Accuracy (%) |
|------------|------------|-------------|---------------------|-----------|--------------------|
| Stage 1    | 9          | 0.0237      | 99.43               | 1.0677    | 72.76              |
| Stage 2    | 8          | 0.4766      | 83.26               | 0.5033    | 83.24              |
| Stage 3    | 6          | 0.3496      | 88.01               | 0.4762    | 84.54              |
| Fine-Tune  | 10         | 0.1070      | 96.40               | 0.3842    | **89.34**          |

## 결과 요약

| 학습 방식                        | 최고 Test Accuracy | 최고 Epoch     | 대응 Loss |
| ---------------------------- | ---------------- | ------------ | ------- |
| **Shuffle + Augmentation**   | **90.21%**       | 16           | 0.3821  |
| **Curriculum + Fine-Tuning** | 89.34%           | 10 (Stage 4) | 0.3842  |

---

## 실험 비교 분석

### 1. **Shuffle + Augmentation 방식**

* 학습 데이터가 **무작위로 고르게 분포**되어 있기 때문에 모든 난이도의 데이터가 **동시에 반복적으로 학습**됨.
* Residual 구조와 **Data Augmentation(Crop + Flip)** 덕분에 모델이 다양한 변형에 강해지고 **빠르게 일반화 성능 확보**.
* 사용한 스케줄러 `ReduceLROnPlateau`는 **test loss가 3번 이상 감소하지 않으면 학습률을 절반으로 줄임** →
  **후반부 학습률이 낮아지면서 모델이 미세한 조정 단계로 넘어가면서 정확도 최대화**.
* Epoch 11\~16 사이에서 **성능이 천천히 수렴**하며 90%를 초과함.

### 2. **Curriculum Learning (Confidence 기반)**

* Stage 1: 쉬운 데이터부터 학습하여 **빠르게 높은 정확도** 도달 (99% Train Acc)
  하지만 Test Loss가 큼 → **overfitting 경향**
* Stage 2\~3: 중간 및 어려운 데이터를 점진적으로 학습하면서 **모델 분포를 확장**
* Fine-Tune 단계: 전체 데이터를 다시 학습하며 **초기화된 파라미터 기반으로 미세조정**
  이 시점에서 `ReduceLROnPlateau`가 적용되어 **학습률이 감소**하면서 정확도가 89.34%까지 향상
* 다만, **Shuffle 대비 다양한 난이도가 반복적으로 제공되지 않아** 최종 성능은 살짝 낮음

---

## ReduceLROnPlateau 스케줄러의 역할

| 역할                      | 적용 방식                                   |
| ----------------------- | --------------------------------------- |
| **성능 정체 구간 감지**         | `test_loss`가 `patience=3` 동안 줄지 않으면 감지  |
| **학습률 감소 (factor=0.5)** | 현재 학습률을 절반으로 줄여, **fine-tuning 정밀도 향상** |
| **정확도 안정화에 기여**         | 학습 후반부에 **loss 진동 없이 수렴** 유도            |

> 특히 Shuffle에서는 스케줄러가 후반부 학습률을 낮춰 **폭발적 정확도 증가 없이 안정적 수렴**을 만들고,
> Curriculum에서는 Fine-Tuning 단계에서 **초기 좋은 파라미터를 미세하게 다듬어** 높은 정확도를 달성하는 데 기여했습니다.

---

## 결론

| 항목               | 분석 요약                                                   |
| ---------------- | ------------------------------------------------------- |
| Shuffle 방식 장점    | 다양한 데이터에 대해 빠르고 고르게 학습 → **최종 정확도 높음**                  |
| Curriculum 방식 장점 | 파라미터 초기화를 유리하게 유도하여 **학습 안정성 확보**                       |
| 공통점 (스케줄러 역할)    | 후반부 학습률 조절로 **loss 안정화 및 과적합 방지**                       |
| 핵심 차이            | Curriculum은 **훈련 과정의 흐름**에 집중, Shuffle은 **데이터 다양성**에 집중 |
