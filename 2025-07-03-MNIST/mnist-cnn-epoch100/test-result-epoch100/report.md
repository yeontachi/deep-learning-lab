# MNIST CNN EPOCH(10 -> 100) 실험 보고서

## 실험 개요
 - **모델 구조** : 2층 CNN(Conv -> ReLU -> MaxPool) + 2층 완전 연결층(FC)
 - **데이터셋** : MNIST(28x28 크기의 흑백 손글씨 숫자 이미지, 총 10개 클래스)
 - **프레임워크** : PyTorch
 - **학습 Epoch 수** : 100
 - **Optimizer** : Adam(Learning rate = 0.001)
 - **활성화 함수** : ReLU
 - **정규화 기법** : 없음
 - **손실 함수** : CrossEntropyLoss(다중 클래스 분류에서 일반적으로 사용)
 - **평가 지표** : CrossEntropyLoss, Accuracy, Confusion Matrix

## 에폭에 따른 loss와 acc 비교 표

| Epoch | Train Loss | Train Acc (%) | Val Loss | Val Acc (%) |
|-------|------------|----------------|----------|--------------|
| 1     | 0.1614     | 94.95          | 0.0651   | 97.92        |
| 2     | 0.0485     | 98.45          | 0.0480   | 98.47        |
| 3     | 0.0325     | 98.99          | 0.0429   | 98.67        |
| 10    | 0.0072     | 99.76          | 0.0495   | 99.03        |
| 20    | 0.0044     | 99.86          | 0.0743   | 98.73        |
| 30    | 0.0017     | 99.95          | 0.0704   | 99.05        |
| 40    | 0.0049     | 99.90          | 0.0898   | 98.96        |
| 50    | 0.0023     | 99.94          | 0.1084   | 98.93        |
| 60    | 0.0019     | 99.94          | 0.1450   | 98.83        |
| 70    | 0.0000     | 100.00         | 0.1265   | 99.01        |
| 80    | 0.0000     | 100.00         | 0.1358   | 99.04        |
| 90    | 0.0000     | 100.00         | 0.1421   | 99.03        |
| 100   | 0.0000     | 100.00         | 0.1490   | 99.08        |

## 결과 보고 - 과적합 현상 분석
모델의 에폭 수를 10에서 100으로 확장하여 학습을 진행한 결과, 훈련 데이터에 대한 성능은 지속적으로 향상된 반면, 검증 데이터에 대한 성능은 일정 지점 이후 정체되거나 오히려 악화되는 양상이 관찰되었다.

에폭 70부터는 Train Loss가 0에 수렴하고, Train Accuracy가 100%에 도달하여 모델이 훈련 데이터에 과하게 적합(Overfitting)된 상태임을 나타낸다. 그러나 같은 구간의 Val Accuracy는 99.01%에서 99.07% 사이를 유지하거나 소폭 하락하고 있으며, Val loss는 에폭 20 기준 0.0743에서 시작하여 점진적으로 증가하여 에폭 100에서 0.1490까지 상승하였다.

이는 명백한 과적합의 지표로, 모델이 훈련 데이터의 패턴을 지나치게 학습하여 새로운 데이터(검증 데이터)에 대한 일반화 성능을 확보하지 못하고 있음을 의미한다. 즉, Train Loss는 계속해서 감소하지만, Val loss는 오히려 증가하는 전형적인 과적합 패턴을 보이고 있다.

한편, 에폭 10에서 11로 넘어갈 때 val loss가 0.0495에서 0.0675로 급상승하는 모습을 보이긴 하나, 이는 단발성 진폭으로 이후 에폭 12~20 구간에서는 Val Loss가 유지되거나 오히려 소폭 감소하는 등 과적합 판단을 내리기에는 이른 시점이다. 실제로 과적합이 명확히 시작된 구간은 에폭 20 이후로, 이 시점부터는 Val Loss가 지속적으로 상승하고, Val Accuracy는 거의 변화가 없는 정체 구간에 들어간다.

따라서 본 실험에서는 Early Stopping을 도입하여 에폭 20~30 사이에서 학습을 종료하는 것이 이상적이며, 검증 성능이 더 이상 향상되지 않거나 오히려 악화되는 시점을 기준으로 학습을 중단함으로써 일반화 성능을 최적화할 수 있다.
