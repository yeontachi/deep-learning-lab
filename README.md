# ğŸ§  Deep Learning Lab

This repository serves as a personal laboratory for deep learning experimentation, research practice, and hands-on model implementation. It includes a variety of neural network experiments using **PyTorch** and other open-source tools, focusing on understanding model behavior through architectural variations, dataset manipulations, and training strategies.

---

## ğŸ§ª Structure & Navigation

Each experiment is stored in a **timestamped directory** for easy chronological tracking.  
The directory naming convention is:

### ğŸ—‚ï¸ Examples:
- `2025-06-30-MNIST` â†’ Experiment using the MNIST dataset
- `2025-07-03-DataAugment` â†’ Experiment with data augmentation techniques
- `2025-07-08-MobileNetV2` â†’ Transfer learning with MobileNetV2
- `2025-07-10-BilliardDetection` â†’ Vision-based object detection practice

> âœ… Each folder includes all relevant code, results, and documentation for the corresponding experiment.

---

## ğŸ“Œ Whatâ€™s Included

- ğŸ“š Model architecture comparisons (MLP, CNN, etc.)
- âš™ï¸ Hyperparameter tuning and optimizer tests
- ğŸ§ª Effects of activation functions and regularization (Dropout, BatchNorm)
- ğŸ”„ Dataset transformations (noise, rotation, scaling)
- ğŸ“Š Result visualization (accuracy plots, confusion matrix, prediction samples)
- ğŸ“ Open-source project analysis and customization

---

## ğŸš€ Frameworks & Tools

- [PyTorch](https://pytorch.org/)
- [Torchvision](https://pytorch.org/vision/)
- [Matplotlib / Seaborn](https://matplotlib.org/)
- [scikit-learn](https://scikit-learn.org/)
- Possibly [OpenCV](https://opencv.org/) or other tools in future tasks

---

## ğŸ¯ Purpose

This lab is designed to:
- Deepen understanding of deep learning theory and practice
- Analyze how different design choices affect model performance
- Prepare for advanced research or academic projects in AI and computer vision

---

## Quantization Roadmap

| ì‹œê¸°                          | ì£¼ìš” ëª¨ë¸                                | ëŒ€í‘œ ì—°êµ¬/ë…¼ë¬¸                                                                                                                                                                                                               | í•µì‹¬ ì•„ì´ë””ì–´                                   | í•™ìŠµ/ì‹¤ìŠµ í¬ì¸íŠ¸                                                                  |
| --------------------------- | ------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------- | -------------------------------------------------------------------------- |
| **2019â€“2020** (CNN ì¤‘ì‹¬)      | MobileNet, ResNet, EfficientNet      | - **HAWQ (2019)**: ë ˆì´ì–´ë³„ ë¯¼ê°ë„ ê¸°ë°˜ mixed-precision<br>- **HAQ (2019)**: RL ê¸°ë°˜ HW-aware quantization<br>- **LSQ (2019)**: step size í•™ìŠµ QAT<br>- **DFQ/ZeroQ (2019â€“2020)**: ë°ì´í„° ì—†ëŠ” PTQ<br>- **AdaRound (2020)**: ì ì‘í˜• ë¼ìš´ë”© PTQ | CNNì„ INT8ë¡œ ì•ˆì •í™”<br>ë¬´ë°ì´í„°Â·í˜¼í•©ì •ë°€ ê¸°ë²• ë“±ì¥         | PyTorch `torch.quantization` í™œìš©<br>ResNet/MobileNet FP32 vs INT8 ì •í™•ë„Â·ì†ë„ ë¹„êµ |
| **2021â€“2022** (ViT ì§„ì…)      | Vision Transformer (ViT, DeiT, Swin) | - **BRECQ (2021)**: ë¸”ë¡ ë‹¨ìœ„ ì¬êµ¬ì„± PTQ<br>- **PTQ4ViT (2022)**: ViT 8-bit ì•ˆì •í™”<br>- **Q-ViT (2022)**: QAT ê¸°ë°˜ ì €ë¹„íŠ¸ ViT                                                                                                         | CNN ëŒ€ë¹„ ë¶„í¬ íŠ¹ì„±ì´ ë‹¤ë¥¸ ViTì— ë§ì¶˜ PTQ/QAT ê¸°ë²•       | HuggingFace ViT ëª¨ë¸ ë¶ˆëŸ¬ì™€ PTQ ì ìš©<br>FP32 vs INT8 ì •í™•ë„ ë¹„êµ                       |
| **2023** (LLMê³¼ì˜ ì ‘ì , ì´ˆì €ë¹„íŠ¸)   | CLIP, ViT+Text                       | - **AWQ (2023)**: í™œì„± ê¸°ë°˜ weight-only 4bit<br>- **QLoRA (2023)**: 4bit + LoRA (LLM ì˜í–¥, VLMì—ë„ ì ìš©)                                                                                                                         | ë¹„ì „-ì–¸ì–´ ëª¨ë¸ì—ë„ 4bit ì ìš© ê°€ëŠ¥ì„± ì—´ë¦¼                 | OpenAI CLIP ëª¨ë¸ì— 4bit quantization ì ìš© ì‹¤ìŠµ                                    |
| **2024** (êµ¬ì¡°ì Â·ë©€í‹°ëª¨ë‹¬ í™•ì¥)      | MobileNetV4, SmolVLA                 | - **Structured Quantization (2024)**: ì±„ë„Â·í–‰ë ¬ ë‹¨ìœ„ ì–‘ìí™”<br>- **MobileNetV4 (2024)**: ì–‘ìí™” ì¹œí™” êµ¬ì¡° ì„¤ê³„<br>- **SmolVLA (2024)**: ì†Œí˜• VLA ëª¨ë¸                                                                                        | ë‹¨ìˆœ ë¹„íŠ¸ ì¶•ì†Œë¥¼ ë„˜ì–´ êµ¬ì¡° ìµœì í™”<br>ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ì—£ì§€ ë””ë°”ì´ìŠ¤ ì ìš© | ONNX/TensorRT ê¸°ë°˜ INT8 ìµœì í™”<br>ëª¨ë°”ì¼/IoT í™˜ê²½ ë°°í¬ ì‹¤ìŠµ                              |
| **2025** (ì´ˆì €ë¹„íŠ¸ ì•ˆì •í™” + HW ê²°í•©) | MicroViT, OpenVLA int4, RepNeXt      | - **APhQ-ViT (CVPR 2025)**: ViT PTQ ì„±ëŠ¥ ê°œì„ <br>- **OpenVLA int4 (2025)**: 4bit ë©€í‹°ëª¨ë‹¬<br>- **RepNeXt (2025)**: HW-aware quantization ì ìš©                                                                                    | 2bit ì´í•˜ ì´ˆì €ë¹„íŠ¸ ì•ˆì •í™”, HWì™€ì˜ ê¸´ë°€í•œ ê²°í•©             | PyTorch+ONNXë¡œ ViT 4bit PTQ/QAT ì‹¤ìŠµ<br>NPU/EdgeTPUì—ì„œ ì¶”ë¡  ì„±ëŠ¥ í™•ì¸                |

---
## ğŸ§‘â€ğŸ’» Author

> JaeHyuck Yeon (ì—°ì¬í˜)  
> Undergraduate Researcher â€” Deep Learning & AI Practice  
> Contact: [yeonjjhh@gmail.com](mailto:yeonjjhh@gmail.com)

---

Feel free to clone the repo, browse through the dated experiment folders, and learn from the process!
